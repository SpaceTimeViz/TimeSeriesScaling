{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## NYISO Load Prediction\n",
    "- Objective: Utilize the NBEATS model to predict NYISO load data for 2023-12-31 using historical data from 2013-01-01 to 2023-12-30.\n",
    "- Zones: `N.Y.C.`, `NORTH`, `CENTRL`\n",
    "- Scaling methods: [definition](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html)\n",
    "     - [`identity`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#std-statistics)\n",
    "     - [`standard`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#std-statistics)\n",
    "     - [`minmax`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#minmax-statistics)\n",
    "     - [`robust`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#robust-statistics)\n",
    "     - `revin`:  learnable normalization parameters are added on top of the usual normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ts_scaler.data.data_handler import DataHandler\n",
    "from ts_scaler.utils.logger import setup_logger\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = setup_logger(level=logging.ERROR)\n",
    "\n",
    "# Fetch data\n",
    "data_handler = DataHandler(logger=logger)\n",
    "df = data_handler.fetch_nyiso_data(local_dir=\"../data\", start_date=\"20130101\", end_date=\"20231231\")\n",
    "\n",
    "# Data preprocessing\n",
    "df = df.drop_duplicates(subset=['time_stamp', 'zone_name'])\n",
    "df = df.dropna()\n",
    "df = df[df['integrated_load'] > 0]\n",
    "\n",
    "# Convert time_stamp to datetime\n",
    "df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "ndf = df.rename(columns={\"time_stamp\": \"ds\", \"integrated_load\": \"y\", \"zone_name\": \"unique_id\"})[\n",
    "    [\"ds\", \"unique_id\", \"y\"]]\n",
    "zones = ndf.unique_id.unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_and_prediction_identity(df_train, df_test, zone):\n",
    "    horizon = len(df_test)\n",
    "    models = [\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='identity'),\n",
    "    ]\n",
    "\n",
    "    nf = NeuralForecast(models=models, freq='H')\n",
    "    nf.fit(df=df_train)\n",
    "    Y_hat_df = nf.predict().reset_index()\n",
    "\n",
    "    Y_hat_df = df_test.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "    # df_train[-(24 * 3):]\n",
    "    plot_df = pd.concat([df_train, Y_hat_df]).drop(\"unique_id\", axis=1).set_index('ds').rename(columns={\n",
    "        'NBEATS': 'NBEATS - Identity',\n",
    "    })\n",
    "    plot_df.index = pd.to_datetime(plot_df.index)\n",
    "    return plot_df\n",
    "\n",
    "\n",
    "def train_and_prediction(df_train, df_test, zone):\n",
    "    horizon = len(df_test)\n",
    "    models = [\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='identity'),\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='standard'),\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='minmax'),\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='robust'),\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='revin'),\n",
    "    ]\n",
    "\n",
    "    nf = NeuralForecast(models=models, freq='H')\n",
    "    nf.fit(df=df_train)\n",
    "    Y_hat_df = nf.predict().reset_index()\n",
    "\n",
    "    Y_hat_df = df_test.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "    # df_train[-(24 * 3):]\n",
    "    plot_df = pd.concat([df_train, Y_hat_df]).drop(\"unique_id\", axis=1).set_index('ds').rename(columns={\n",
    "        'NBEATS': 'NBEATS - Identity',\n",
    "        'NBEATS1': 'NBEATS - Standard',\n",
    "        'NBEATS2': 'NBEATS - MinMax',\n",
    "        'NBEATS3': 'NBEATS - Robust',\n",
    "        'NBEATS4': 'NBEATS - Revin',\n",
    "    })\n",
    "    plot_df.index = pd.to_datetime(plot_df.index)\n",
    "    return plot_df\n",
    "\n",
    "\n",
    "def process_and_predict(df_train, df_test, zone):\n",
    "    # Ensure data types are consistent\n",
    "    df_train = df_train.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "    df_train['unique_id'] = df_train['unique_id'].astype(str)\n",
    "    df_test['unique_id'] = df_test['unique_id'].astype(str)\n",
    "\n",
    "    df_train['ds'] = pd.to_datetime(df_train['ds'])\n",
    "    df_test['ds'] = pd.to_datetime(df_test['ds'])\n",
    "\n",
    "    df_train['y'] = df_train['y'].astype(float)\n",
    "    df_test['y'] = df_test['y'].astype(float)\n",
    "\n",
    "    # Reset index and prepare the DataFrame for residuals\n",
    "    df_train_residual = df_train[[\"ds\", \"residuals\", \"unique_id\"]]\n",
    "    df_train_residual = df_train_residual.rename(columns={\"residuals\": \"y\"})[[\"ds\", \"unique_id\", \"y\"]]\n",
    "    df_test_residual = df_test[[\"ds\", \"residuals\", \"unique_id\"]]\n",
    "    df_test_residual = df_test_residual.rename(columns={\"residuals\": \"y\"})[[\"ds\", \"unique_id\", \"y\"]]\n",
    "\n",
    "    # Perform predictions using the identity method\n",
    "    pca_prediction_df = train_and_prediction_identity(df_train_residual, df_test_residual, zone)\n",
    "\n",
    "    df_combined = pd.concat([df_train, df_test])\n",
    "\n",
    "    # Ensure the column types for the df_combined DataFrame\n",
    "    df_combined['unique_id'] = df_combined['unique_id'].astype(str)\n",
    "    df_combined['ds'] = pd.to_datetime(df_combined['ds'])\n",
    "    df_combined['reconstructed'] = df_combined['reconstructed'].astype(float)\n",
    "\n",
    "    # Remove duplicate indices\n",
    "    df_combined = df_combined.drop_duplicates(subset=['ds', 'unique_id'])\n",
    "    df_train_residual = df_train_residual.drop_duplicates(subset=['ds', 'unique_id'])\n",
    "    df_test_residual = df_test_residual.drop_duplicates(subset=['ds', 'unique_id'])\n",
    "\n",
    "    # Perform predictions using the general method\n",
    "    all_prediction_df = train_and_prediction(df_train[[\"ds\", \"y\", \"unique_id\"]],\n",
    "                                             df_test[[\"ds\", \"y\", \"unique_id\"]],\n",
    "                                             zone)\n",
    "\n",
    "    # Ensure 'ds' and 'unique_id' are present in all_prediction_df\n",
    "    all_prediction_df['ds'] = df_combined['ds'].values\n",
    "    all_prediction_df['unique_id'] = df_combined['unique_id'].values\n",
    "\n",
    "    pca_prediction_df['ds'] = df_combined['ds'].values\n",
    "    pca_prediction_df['unique_id'] = df_combined['unique_id'].values\n",
    "\n",
    "    # Ensure alignment before addition\n",
    "    all_prediction_df = all_prediction_df.set_index(['ds', 'unique_id'])\n",
    "    pca_prediction_df = pca_prediction_df.set_index(['ds', 'unique_id'])\n",
    "    df_combined = df_combined.set_index(['ds', 'unique_id'])\n",
    "\n",
    "    # Perform the addition with alignment\n",
    "    all_prediction_df['NBEATS - PCA'] = pca_prediction_df['NBEATS - Identity'] + df_combined['reconstructed']\n",
    "\n",
    "    all_prediction_df = all_prediction_df.reset_index()\n",
    "    all_prediction_df[\"unique_id\"] = zone\n",
    "\n",
    "    return all_prediction_df.set_index('ds')\n",
    "\n",
    "\n",
    "def add_residuals_and_reconstructed(df, unique_id='N.Y.C.', interpolation_method='linear'):\n",
    "    # Filter the data for the specified unique_id\n",
    "    df_filtered = df[df['unique_id'] == unique_id].copy()\n",
    "\n",
    "    # Ensure 'ds' is a datetime type and set it as index\n",
    "    if 'ds' not in df_filtered.columns:\n",
    "        raise KeyError(\"'ds' column not found in the DataFrame\")\n",
    "\n",
    "    df_filtered['ds'] = pd.to_datetime(df_filtered['ds'])\n",
    "    df_filtered.set_index('ds', inplace=True)\n",
    "\n",
    "    # Ensure the index is continuous and fills in any missing hourly data with NaNs\n",
    "    df_filtered = df_filtered.asfreq('H')\n",
    "\n",
    "    # Interpolate missing values\n",
    "    df_filtered['y'] = df_filtered['y'].interpolate(method=interpolation_method)\n",
    "\n",
    "    # Check if there is at least one Monday and one Sunday in the dataset\n",
    "    mondays = df_filtered.index[df_filtered.index.weekday == 0]\n",
    "    sundays = df_filtered.index[df_filtered.index.weekday == 6]\n",
    "    if len(mondays) == 0 or len(sundays) == 0:\n",
    "        logging.warning(\"The dataset does not contain the required Monday and Sunday dates for processing.\")\n",
    "        return df_filtered\n",
    "\n",
    "    # Define the training and test periods\n",
    "    last_sunday = sundays[-1]\n",
    "    last_monday = last_sunday - pd.DateOffset(days=6, hours=23)\n",
    "    df_test = df_filtered[last_monday:last_monday + pd.DateOffset(hours=23)]\n",
    "    last_train_sunday = last_monday - pd.DateOffset(days=1)\n",
    "    df_train = df_filtered[mondays[0]:last_train_sunday + pd.DateOffset(hours=23)]\n",
    "\n",
    "    # Reshape the training data into weekly format (7 days per week, 24 hours per day)\n",
    "    reshaped_train_data = df_train['y'].values.reshape(-1, 7 * 24)\n",
    "\n",
    "    # Standardize the training data\n",
    "    scaler = StandardScaler()\n",
    "    reshaped_scaled = scaler.fit_transform(reshaped_train_data)\n",
    "\n",
    "    # Apply PCA to the training data\n",
    "    pca = PCA(n_components=20)  # Using only the first principal component\n",
    "    pca.fit(reshaped_scaled)\n",
    "\n",
    "    # Reconstruct the training data using the first principal component\n",
    "    principal_components_train = pca.transform(reshaped_scaled)\n",
    "    reconstructed_train_data = pca.inverse_transform(principal_components_train)\n",
    "    reconstructed_train_data_original_scale = scaler.inverse_transform(reconstructed_train_data).flatten()\n",
    "\n",
    "    # Calculate the residuals for the training data\n",
    "    residuals_train = df_train['y'].values - reconstructed_train_data_original_scale\n",
    "\n",
    "    # Add residuals and reconstructed data as new columns in the training DataFrame\n",
    "    df_train['residuals'] = residuals_train\n",
    "    df_train['reconstructed'] = reconstructed_train_data_original_scale\n",
    "\n",
    "    # Handle the test data\n",
    "    test_data = df_test['y'].values\n",
    "\n",
    "    # Standardize the test data using the mean and std from the training data\n",
    "    reshaped_test_scaled = (test_data - scaler.mean_[:24]) / scaler.scale_[:24]\n",
    "\n",
    "    # Pad the test data to match the number of features expected by PCA (168 features)\n",
    "    padded_test_scaled = np.zeros((1, 168))\n",
    "    padded_test_scaled[0, :24] = reshaped_test_scaled\n",
    "\n",
    "    # Reconstruct the test data using the same PCA\n",
    "    principal_components_test = pca.transform(padded_test_scaled)\n",
    "    reconstructed_padded_test_data = pca.inverse_transform(principal_components_test)\n",
    "    reconstructed_test_data_original_scale = (reconstructed_padded_test_data[:, :24] * scaler.scale_[\n",
    "                                                                                       :24]) + scaler.mean_[:24]\n",
    "\n",
    "    # Calculate the residuals for the test data\n",
    "    residuals_test = df_test['y'].values - reconstructed_test_data_original_scale.flatten()\n",
    "\n",
    "    # Add residuals and reconstructed data as new columns in the test DataFrame\n",
    "    df_test['residuals'] = residuals_test\n",
    "    df_test['reconstructed'] = reconstructed_test_data_original_scale.flatten()\n",
    "\n",
    "    return df_train, df_test\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_dfs = {}\n",
    "test_dfs = {}\n",
    "\n",
    "for zone in zones:\n",
    "    train_dfs[zone], test_dfs[zone] = add_residuals_and_reconstructed(ndf, zone)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_prediction_dfs = {}\n",
    "for zone in zones:\n",
    "    all_prediction_dfs[zone] = process_and_predict(train_dfs[zone], test_dfs[zone], zone)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initial zone\n",
    "# Save filtered data to pickle files\n",
    "with open('train_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dfs, f)\n",
    "with open('test_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dfs, f)\n",
    "\n",
    "with open('all_prediction_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_prediction_dfs, f)\n",
    "\n",
    "with open('zones.pkl', 'wb') as f:\n",
    "    pickle.dump(zones, f)\n",
    "\n",
    "print(\"Data saved to pickle files successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-scaler",
   "language": "python",
   "name": "ts-scaler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
