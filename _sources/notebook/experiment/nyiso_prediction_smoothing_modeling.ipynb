{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## NYISO Load Prediction\n",
    "- Objective: Utilize the NBEATS model to predict NYISO load data for 2023-12-31 using historical data from 2013-01-01 to 2023-12-30.\n",
    "- Zones: `N.Y.C.`, `NORTH`, `CENTRL`\n",
    "- Scaling methods: [definition](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html)\n",
    "     - [`identity`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#std-statistics)\n",
    "     - [`standard`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#std-statistics)\n",
    "     - [`minmax`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#minmax-statistics)\n",
    "     - [`robust`](https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html#robust-statistics)\n",
    "     - `revin`:  learnable normalization parameters are added on top of the usual normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from ts_scaler.data.data_handler import DataHandler\n",
    "from ts_scaler.utils.logger import setup_logger\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = setup_logger(level=logging.ERROR)\n",
    "\n",
    "# Fetch data\n",
    "data_handler = DataHandler(logger=logger)\n",
    "df = data_handler.fetch_nyiso_data(local_dir=\"../data\", start_date=\"20130101\", end_date=\"20231231\")\n",
    "\n",
    "# Data preprocessing\n",
    "df = df.drop_duplicates(subset=['time_stamp', 'zone_name'])\n",
    "df = df.dropna()\n",
    "df = df[df['integrated_load'] > 0]\n",
    "\n",
    "# Convert time_stamp to datetime\n",
    "df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "ndf = df.rename(columns={\"time_stamp\": \"ds\", \"integrated_load\": \"y\", \"zone_name\": \"unique_id\"})[\n",
    "    [\"ds\", \"unique_id\", \"y\"]]\n",
    "zones = ndf.unique_id.unique()\n",
    "sigma = 448"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ndf[ndf.unique_id == \"CAPITL\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_train_test_split(df, train_length, test_length):\n",
    "    # Ensure the DataFrame is sorted by date\n",
    "    df = df.sort_values(by='ds')\n",
    "\n",
    "    test_start_index = np.random.randint(train_length - 1, len(df))\n",
    "\n",
    "    # Select the test data starting from the chosen index\n",
    "    df_test = df.iloc[test_start_index: (test_start_index + test_length)]\n",
    "\n",
    "    # Select the training data as the K timepoints before the test data\n",
    "    df_train = df.iloc[test_start_index - train_length:test_start_index]\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def generate_train_test_pairs(df, unique_ids, train_length, test_length, num_pairs=100):\n",
    "    all_train_test_pairs = {}\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        pairs = []\n",
    "        df_unique = df[df.unique_id == unique_id]\n",
    "\n",
    "        for _ in range(num_pairs):\n",
    "            df_train, df_test = create_train_test_split(df_unique, train_length, test_length)\n",
    "            pairs.append((df_train, df_test))\n",
    "\n",
    "        all_train_test_pairs[unique_id] = pairs\n",
    "\n",
    "    return all_train_test_pairs\n",
    "\n",
    "\n",
    "# Define the number of previous timepoints to use as training data\n",
    "train_length = 200 * 24  # Example: 200 days of hourly data\n",
    "test_length = 1 * 24  # Example: 1 day of hourly data\n",
    "\n",
    "# Assume `ndf` is your DataFrame and contains a 'unique_id' column\n",
    "unique_ids = ndf['unique_id'].unique()\n",
    "\n",
    "# Generate 100 train/test pairs for each unique_id\n",
    "all_train_test_pairs = generate_train_test_pairs(ndf, unique_ids, train_length, test_length, num_pairs=100)\n",
    "\n",
    "# Example: Access the first train/test pair for a specific unique_id\n",
    "example_unique_id = unique_ids[0]\n",
    "example_train, example_test = all_train_test_pairs[example_unique_id][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, sigma=2):\n",
    "        self.sigma = sigma\n",
    "        self.smoothed_y = None\n",
    "        self.std_y = None\n",
    "\n",
    "    def fit(self, y):\n",
    "        # Fill NaN values before fitting\n",
    "        y = pd.Series(y).ffill().bfill().values\n",
    "\n",
    "        # Apply Gaussian kernel smoothing to calculate smoothed values\n",
    "        self.smoothed_y = gaussian_filter1d(y, sigma=self.sigma)\n",
    "\n",
    "        # Compute the deviations from the smoothed line\n",
    "        deviations = y - self.smoothed_y\n",
    "\n",
    "        # Apply Gaussian kernel smoothing to the squared deviations to estimate std\n",
    "        smoothed_squared_deviations = gaussian_filter1d(deviations ** 2, sigma=self.sigma)\n",
    "        self.std_y = np.sqrt(smoothed_squared_deviations)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        # Fill NaN values before transforming\n",
    "        y = pd.Series(y).ffill().bfill().values\n",
    "\n",
    "        if self.smoothed_y is None or self.std_y is None:\n",
    "            raise RuntimeError(\"The normalizer must be fitted before calling transform.\")\n",
    "\n",
    "        smoothed_y_partial = self.smoothed_y[-len(y):]\n",
    "        std_y_partial = self.std_y[-len(y):]\n",
    "\n",
    "        # Handle any NaN or zero values in std_y_partial to avoid extreme values\n",
    "        std_y_partial = np.where(np.isnan(std_y_partial) | (std_y_partial == 0), 1e-6, std_y_partial)\n",
    "\n",
    "        normalized_y = (y - smoothed_y_partial) / std_y_partial\n",
    "\n",
    "        # Forward-fill any remaining NaN values in normalized_y\n",
    "        normalized_y = pd.Series(normalized_y).ffill().values\n",
    "\n",
    "        return normalized_y\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        self.fit(y)\n",
    "        return self.transform(y)\n",
    "\n",
    "    def inverse_transform(self, normalized_y):\n",
    "        if self.smoothed_y is None or self.std_y is None:\n",
    "            raise RuntimeError(\"The normalizer must be fitted before calling inverse_transform.\")\n",
    "\n",
    "        smoothed_y_partial = self.smoothed_y[-len(normalized_y):]\n",
    "        std_y_partial = self.std_y[-len(normalized_y):]\n",
    "\n",
    "        denormalized_y = normalized_y * std_y_partial + smoothed_y_partial\n",
    "        return denormalized_y\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize a dictionary to store normalizers for each unique_id\n",
    "normalizers = {}\n",
    "\n",
    "# Initialize lists to store train and test DataFrames\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Loop through each unique_id group\n",
    "for unique_id, group in ndf.groupby('unique_id'):\n",
    "    # Sort the group by date to ensure correct splitting\n",
    "    group = group.sort_values(by='ds')\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data = group.iloc[:-168]\n",
    "    test_data = group.iloc[-168:]\n",
    "\n",
    "    # Initialize a new normalizer for the training data\n",
    "    #normalizer = Normalizer(window_size=90, polyorder=2)\n",
    "    normalizer = Normalizer(sigma=sigma)\n",
    "    # Fit the normalizer on the training data only\n",
    "    train_data['normalized_y'] = normalizer.fit_transform(train_data['y'].values)\n",
    "\n",
    "    # Transform the test data using the fitted normalizer\n",
    "    test_data['normalized_y'] = normalizer.transform(test_data['y'].values)\n",
    "\n",
    "    # Store the normalizer in the dictionary\n",
    "    normalizers[unique_id] = normalizer\n",
    "\n",
    "    # Append the processed train and test data to their respective lists\n",
    "    train_dfs.append(train_data)\n",
    "    test_dfs.append(test_data)\n",
    "\n",
    "# Concatenate the processed data back into single DataFrames\n",
    "train_df = pd.concat(train_dfs)\n",
    "test_df = pd.concat(test_dfs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_and_prediction_smoothing(df_train, df_test, normalizer):\n",
    "    horizon = len(df_test)\n",
    "    models = [\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='identity'),\n",
    "    ]\n",
    "\n",
    "    nf = NeuralForecast(models=models, freq='H')\n",
    "    nf.fit(df=df_train)\n",
    "    Y_hat_df = nf.predict().reset_index()\n",
    "    Y_hat_df['NBEATS'] = normalizer.inverse_transform(Y_hat_df['NBEATS'].values)\n",
    "    Y_hat_df = df_test.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "    # df_train[-(24 * 3):]\n",
    "    plot_df = pd.concat([df_train, Y_hat_df]).drop(\"unique_id\", axis=1).set_index('ds').rename(columns={\n",
    "        'NBEATS': 'NBEATS - Smoothing',\n",
    "    })\n",
    "    plot_df.index = pd.to_datetime(plot_df.index)\n",
    "    return plot_df\n",
    "\n",
    "\n",
    "def train_and_prediction(df_train, df_test):\n",
    "    horizon = len(df_test)\n",
    "    models = [\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='identity'),\n",
    "        NBEATS(input_size=len(df_test) * 7, h=horizon, max_steps=500, scaler_type='revin'),\n",
    "    ]\n",
    "\n",
    "    nf = NeuralForecast(models=models, freq='H')\n",
    "    nf.fit(df=df_train)\n",
    "    Y_hat_df = nf.predict().reset_index()\n",
    "\n",
    "    Y_hat_df = df_test.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n",
    "    # df_train[-(24 * 3):]\n",
    "    plot_df = pd.concat([df_train, Y_hat_df]).drop(\"unique_id\", axis=1).set_index('ds').rename(columns={\n",
    "        'NBEATS': 'NBEATS - Identity',\n",
    "        'NBEATS1': 'NBEATS - Revin',\n",
    "    })\n",
    "    plot_df.index = pd.to_datetime(plot_df.index)\n",
    "    return plot_df\n",
    "\n",
    "\n",
    "def process_and_predict(df_train, df_test, zone, normalizer):\n",
    "    # Ensure data types are consistent\n",
    "    df_train = df_train.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "    df_train['unique_id'] = df_train['unique_id'].astype(str)\n",
    "    df_test['unique_id'] = df_test['unique_id'].astype(str)\n",
    "\n",
    "    df_train['ds'] = pd.to_datetime(df_train['ds'])\n",
    "    df_test['ds'] = pd.to_datetime(df_test['ds'])\n",
    "\n",
    "    df_train['y'] = df_train['y'].astype(float)\n",
    "    df_test['y'] = df_test['y'].astype(float)\n",
    "\n",
    "    # Reset index and prepare the DataFrame for residuals\n",
    "    df_train_smoothing = df_train[[\"ds\", \"normalized_y\", \"unique_id\"]]\n",
    "    df_train_smoothing = df_train_smoothing.rename(columns={\"normalized_y\": \"y\"})[[\"ds\", \"unique_id\", \"y\"]]\n",
    "    df_test_smoothing = df_test[[\"ds\", \"normalized_y\", \"unique_id\"]]\n",
    "    df_test_smoothing = df_test_smoothing.rename(columns={\"normalized_y\": \"y\"})[[\"ds\", \"unique_id\", \"y\"]]\n",
    "\n",
    "    # Perform predictions using the general method\n",
    "    all_prediction_df = train_and_prediction(df_train[[\"ds\", \"y\", \"unique_id\"]],\n",
    "                                             df_test[[\"ds\", \"y\", \"unique_id\"]],\n",
    "                                             )\n",
    "    df_combined = pd.concat([df_train, df_test])\n",
    "\n",
    "    # Ensure the column types for the df_combined DataFrame\n",
    "    df_combined['unique_id'] = df_combined['unique_id'].astype(str)\n",
    "    df_combined['ds'] = pd.to_datetime(df_combined['ds'])\n",
    "    df_combined = df_combined.drop_duplicates(subset=['ds', 'unique_id'])\n",
    "    # Perform predictions using the identity method\n",
    "    all_prediction_df['NBEATS - Smoothing'] = \\\n",
    "        train_and_prediction_smoothing(df_train_smoothing, df_test_smoothing, normalizer)['NBEATS - Smoothing']\n",
    "\n",
    "    # Ensure 'ds' and 'unique_id' are present in all_prediction_df\n",
    "    all_prediction_df['ds'] = df_combined['ds'].values\n",
    "    all_prediction_df['unique_id'] = df_combined['unique_id'].values\n",
    "    all_prediction_df[\"unique_id\"] = zone\n",
    "\n",
    "    return all_prediction_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "zones = ndf.unique_id.unique()\n",
    "all_prediction_dfs = defaultdict(list)\n",
    "normalizers = defaultdict(list)\n",
    "\n",
    "for zone in zones:\n",
    "    for train_df, test_df in all_train_test_pairs[zone]:\n",
    "        normalizer = Normalizer(sigma=sigma)\n",
    "        # Fit the normalizer on the training data only\n",
    "        train_df['normalized_y'] = normalizer.fit_transform(train_df['y'].values)\n",
    "\n",
    "        # Transform the test data using the fitted normalizer\n",
    "        test_df['normalized_y'] = normalizer.transform(test_df['y'].values)\n",
    "\n",
    "        # Store the normalizer in the dictionary\n",
    "        normalizers[zone].append(normalizer)\n",
    "\n",
    "        predicted_df = process_and_predict(train_df, test_df, zone, normalizer)\n",
    "        all_prediction_dfs[zone].append(predicted_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initial zone\n",
    "# Save filtered data to pickle files\n",
    "with open('all_prediction_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_prediction_dfs, f)\n",
    "with open('normalizers.pkl', 'wb') as f:\n",
    "    pickle.dump(normalizers, f)\n",
    "\n",
    "with open('all_train_test_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_train_test_pairs, f)\n",
    "\n",
    "print(\"Data saved to pickle files successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-scaler",
   "language": "python",
   "name": "ts-scaler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
